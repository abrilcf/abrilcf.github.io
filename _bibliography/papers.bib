---
---

@article{CORONAFIGUEROA2019100268,
  title = {A portable prototype for diagnosing fetal arrhythmia},
  journal = {Informatics in Medicine Unlocked},
  volume = {17},
  pages = {100268},
  year = {2019},
  issn = {2352-9148},
  doi = {https://doi.org/10.1016/j.imu.2019.100268},
  url = {https://www.sciencedirect.com/science/article/pii/S2352914819301893},
  author = {Abril Corona-Figueroa},
  bibtex_show={true},
  pdf={https://www.sciencedirect.com/science/article/pii/S2352914819301893},
  abstract = {Expert knowledge and proper techniques are necessary for diagnosing and treating fetal cardiac disease. Fetal arrhythmia represents an important part of fetal cardiac disease as it conveys abnormalities in the electrophysiological state of the heart. Arrhythmias can be caused by rate or rhythm alterations or by abnormalities in QRS or QT intervals. Fetal echocardiography has been the gold standard for pediatric cardiologists since it assesses multiple aspects of fetal cardiovascular pathology. In parallel, artificial intelligence techniques are being designed to provide additional support to experts in the medical field in terms of detection and accuracy. The present manuscript describes the development of a prototype meant to be an auxiliary for the detection of fetal arrhythmias by analyzing the fetal heart rate (FHR) and its variability. It consists of a portable electrocardiograph and a mobile application that, together, extract the fetal electrocardiogram for its analysis to finally render a first diagnosis. Conducted tests with synthetic signals based on clinical fetal arrhythmia provided a detection rate of 88.88\%. Results from tests with pregnant women may indicate that the proposed prototype can be used to render a first diagnosis without entirely depending on expert assistance. This prototype might be helpful to perform routine check-ups in the second or third trimester in pathological pregnancies associated with high risk of onset/progression of fetal arrhythmias. In addition, medical practitioners could use it to corroborate a given diagnosis or in scenarios where there is an insufficiency of human experts to attend a high population of patients.}
}

@INPROCEEDINGS{9871757,
  author={Corona-Figueroa, Abril and Frawley, Jonathan and Bond-Taylor, Sam and Bethapudi, Sarath and Shum, Hubert P. H. and Willcocks, Chris G.},
  booktitle={2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)}, 
  title={MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray}, 
  year={2022},
  volume={},
  number={},
  pages={3843-3848},
  doi={10.1109/EMBC48229.2022.9871757},
  bibtex_show={true},
  code={https://github.com/abrilcf/mednerf},
  pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9871757},
  dimensions={false},
  abstract={Computed tomography (CT) is an effective med-ical imaging modality, widely used in the field of clinical medicine for the diagnosis of various pathologies. Advances in Multidetector CT imaging technology have enabled additional functionalities, including generation of thin slice multi planar cross-sectional body imaging and 3D reconstructions. However, this involves patients being exposed to a considerable dose of ionising radiation. Excessive ionising radiation can lead to deterministic and harmful effects on the body. This paper proposes a Deep Learning model that learns to reconstruct CT projections from a few or even a single-view X-ray. This is based on a novel architecture that builds from neural radiance fields, which learns a continuous representation of CT scans by disentangling the shape and volumetric depth of surface and internal anatomical structures from 2D images. Our model is trained on chest and knee datasets, and we demonstrate qual-itative and quantitative high-fidelity renderings and compare our approach to other recent radiance field-based methods. Our code and link to our datasets are available at https://qithub.com/abrilcf/mednerf Clinical relevance- Our model is able to infer the anatomical 3D structure from a few or a single-view X-ray showing future potential for reduced ionising radiation exposure during the imaging process.}
}

@inproceedings{coronafigueroaa23unaligned,
 author={Corona-Figueroa, Abril and Bond-Taylor, Sam and Bhowmik, Neelanjan and Gaus, Yona Falinie A. and Breckon, Toby P. and Shum, Hubert P. H. and Willcocks, Chris G.},
 booktitle={Proceedings of the 2023 IEEE/CVF International Conference on Computer Vision},
 series={ICCV '23},
 title={Unaligned 2D to 3D Translation with Conditional Vector-Quantized Code Diffusion using Transformers},
 year={2023},
 month={10},
 publisher={IEEE/CVF},
 location={Paris, France},
 bibtex_show={true},
 pdf={https://arxiv.org/pdf/2308.14152.pdf},
 abstract={Generating 3D images of complex objects conditionally from a few 2D views is a difficult synthesis problem, compounded by issues such as domain gap and geometric misalignment. For instance, a unified framework such as Generative Adversarial Networks cannot achieve this unless they explicitly define both a domain-invariant and geometric-invariant joint latent distribution, whereas Neural Radiance Fields are generally unable to handle both issues as they optimize at the pixel level. By contrast, we propose a simple and novel 2D to 3D synthesis approach based on conditional diffusion with vector-quantized codes. Operating in an information-rich code space enables highresolution 3D synthesis via full-coverage attention across the views. Specifically, we generate the 3D codes, eg for CT images, conditional on previously generated 3D codes and the entire codebook of two 2D views (eg 2D X-rays). Qualitative and quantitative results demonstrate state-ofthe-art performance over specialized methods across varied evaluation criteria, including fidelity metrics such as density and coverage and distortion metrics for two datasets of complex volumetric imagery found in real-world scenarios.}
}